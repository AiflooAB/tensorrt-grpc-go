// Code generated by protoc-gen-go. DO NOT EDIT.
// source: src/core/grpc_service.proto

package nvidia_inferenceserver

import (
	context "context"
	fmt "fmt"
	proto "github.com/golang/protobuf/proto"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
	math "math"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

//@@
//@@.. cpp:var:: message StatusRequest
//@@
//@@   Request message for Status gRPC endpoint.
//@@
type StatusRequest struct {
	//@@
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The specific model status to be returned. If empty return status
	//@@     for all models.
	//@@
	ModelName            string   `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *StatusRequest) Reset()         { *m = StatusRequest{} }
func (m *StatusRequest) String() string { return proto.CompactTextString(m) }
func (*StatusRequest) ProtoMessage()    {}
func (*StatusRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{0}
}

func (m *StatusRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StatusRequest.Unmarshal(m, b)
}
func (m *StatusRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StatusRequest.Marshal(b, m, deterministic)
}
func (m *StatusRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StatusRequest.Merge(m, src)
}
func (m *StatusRequest) XXX_Size() int {
	return xxx_messageInfo_StatusRequest.Size(m)
}
func (m *StatusRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_StatusRequest.DiscardUnknown(m)
}

var xxx_messageInfo_StatusRequest proto.InternalMessageInfo

func (m *StatusRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

//@@
//@@.. cpp:var:: message StatusResponse
//@@
//@@   Response message for Status gRPC endpoint.
//@@
type StatusResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	//@@
	//@@  .. cpp:var:: ServerStatus server_status
	//@@
	//@@     The server and model status.
	//@@
	ServerStatus         *ServerStatus `protobuf:"bytes,2,opt,name=server_status,json=serverStatus,proto3" json:"server_status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}      `json:"-"`
	XXX_unrecognized     []byte        `json:"-"`
	XXX_sizecache        int32         `json:"-"`
}

func (m *StatusResponse) Reset()         { *m = StatusResponse{} }
func (m *StatusResponse) String() string { return proto.CompactTextString(m) }
func (*StatusResponse) ProtoMessage()    {}
func (*StatusResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{1}
}

func (m *StatusResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_StatusResponse.Unmarshal(m, b)
}
func (m *StatusResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_StatusResponse.Marshal(b, m, deterministic)
}
func (m *StatusResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StatusResponse.Merge(m, src)
}
func (m *StatusResponse) XXX_Size() int {
	return xxx_messageInfo_StatusResponse.Size(m)
}
func (m *StatusResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_StatusResponse.DiscardUnknown(m)
}

var xxx_messageInfo_StatusResponse proto.InternalMessageInfo

func (m *StatusResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

func (m *StatusResponse) GetServerStatus() *ServerStatus {
	if m != nil {
		return m.ServerStatus
	}
	return nil
}

//@@
//@@.. cpp:var:: message ProfileRequest
//@@
//@@   Request message for Profile gRPC endpoint.
//@@
type ProfileRequest struct {
	//@@
	//@@  .. cpp:var:: string cmd
	//@@
	//@@     The requested profiling action: 'start' requests that GPU
	//@@     profiling be enabled on all GPUs controlled by the inference
	//@@     server; 'stop' requests that GPU profiling be disabled on all GPUs
	//@@     controlled by the inference server.
	//@@
	Cmd                  string   `protobuf:"bytes,1,opt,name=cmd,proto3" json:"cmd,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *ProfileRequest) Reset()         { *m = ProfileRequest{} }
func (m *ProfileRequest) String() string { return proto.CompactTextString(m) }
func (*ProfileRequest) ProtoMessage()    {}
func (*ProfileRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{2}
}

func (m *ProfileRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProfileRequest.Unmarshal(m, b)
}
func (m *ProfileRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProfileRequest.Marshal(b, m, deterministic)
}
func (m *ProfileRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProfileRequest.Merge(m, src)
}
func (m *ProfileRequest) XXX_Size() int {
	return xxx_messageInfo_ProfileRequest.Size(m)
}
func (m *ProfileRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_ProfileRequest.DiscardUnknown(m)
}

var xxx_messageInfo_ProfileRequest proto.InternalMessageInfo

func (m *ProfileRequest) GetCmd() string {
	if m != nil {
		return m.Cmd
	}
	return ""
}

//@@
//@@.. cpp:var:: message ProfileResponse
//@@
//@@   Response message for Profile gRPC endpoint.
//@@
type ProfileResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus        *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}       `json:"-"`
	XXX_unrecognized     []byte         `json:"-"`
	XXX_sizecache        int32          `json:"-"`
}

func (m *ProfileResponse) Reset()         { *m = ProfileResponse{} }
func (m *ProfileResponse) String() string { return proto.CompactTextString(m) }
func (*ProfileResponse) ProtoMessage()    {}
func (*ProfileResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{3}
}

func (m *ProfileResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_ProfileResponse.Unmarshal(m, b)
}
func (m *ProfileResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_ProfileResponse.Marshal(b, m, deterministic)
}
func (m *ProfileResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ProfileResponse.Merge(m, src)
}
func (m *ProfileResponse) XXX_Size() int {
	return xxx_messageInfo_ProfileResponse.Size(m)
}
func (m *ProfileResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_ProfileResponse.DiscardUnknown(m)
}

var xxx_messageInfo_ProfileResponse proto.InternalMessageInfo

func (m *ProfileResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

//@@
//@@.. cpp:var:: message HealthRequest
//@@
//@@   Request message for Health gRPC endpoint.
//@@
type HealthRequest struct {
	//@@
	//@@  .. cpp:var:: string mode
	//@@
	//@@     The requested health action: 'live' requests the liveness
	//@@     state of the inference server; 'ready' requests the readiness state
	//@@     of the inference server.
	//@@
	Mode                 string   `protobuf:"bytes,1,opt,name=mode,proto3" json:"mode,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *HealthRequest) Reset()         { *m = HealthRequest{} }
func (m *HealthRequest) String() string { return proto.CompactTextString(m) }
func (*HealthRequest) ProtoMessage()    {}
func (*HealthRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{4}
}

func (m *HealthRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_HealthRequest.Unmarshal(m, b)
}
func (m *HealthRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_HealthRequest.Marshal(b, m, deterministic)
}
func (m *HealthRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_HealthRequest.Merge(m, src)
}
func (m *HealthRequest) XXX_Size() int {
	return xxx_messageInfo_HealthRequest.Size(m)
}
func (m *HealthRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_HealthRequest.DiscardUnknown(m)
}

var xxx_messageInfo_HealthRequest proto.InternalMessageInfo

func (m *HealthRequest) GetMode() string {
	if m != nil {
		return m.Mode
	}
	return ""
}

//@@
//@@.. cpp:var:: message HealthResponse
//@@
//@@   Response message for Health gRPC endpoint.
//@@
type HealthResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	//@@
	//@@  .. cpp:var:: bool health
	//@@
	//@@     The result of the request. True indicates the inference server is
	//@@     live/ready, false indicates the inference server is not live/ready.
	//@@
	Health               bool     `protobuf:"varint,2,opt,name=health,proto3" json:"health,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *HealthResponse) Reset()         { *m = HealthResponse{} }
func (m *HealthResponse) String() string { return proto.CompactTextString(m) }
func (*HealthResponse) ProtoMessage()    {}
func (*HealthResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{5}
}

func (m *HealthResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_HealthResponse.Unmarshal(m, b)
}
func (m *HealthResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_HealthResponse.Marshal(b, m, deterministic)
}
func (m *HealthResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_HealthResponse.Merge(m, src)
}
func (m *HealthResponse) XXX_Size() int {
	return xxx_messageInfo_HealthResponse.Size(m)
}
func (m *HealthResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_HealthResponse.DiscardUnknown(m)
}

var xxx_messageInfo_HealthResponse proto.InternalMessageInfo

func (m *HealthResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

func (m *HealthResponse) GetHealth() bool {
	if m != nil {
		return m.Health
	}
	return false
}

//@@
//@@.. cpp:var:: message InferRequest
//@@
//@@   Request message for Infer gRPC endpoint.
//@@
type InferRequest struct {
	//@@  .. cpp:var:: string model_name
	//@@
	//@@     The name of the model to use for inferencing.
	//@@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	//@@  .. cpp:var:: int64 version
	//@@
	//@@     The version of the model to use for inference. If -1
	//@@     the latest/most-recent version of the model is used.
	//@@
	ModelVersion int64 `protobuf:"varint,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	//@@  .. cpp:var:: InferRequestHeader meta_data
	//@@
	//@@     Meta-data for the request profiling input tensors and requesting
	//@@     output tensors.
	//@@
	MetaData *InferRequestHeader `protobuf:"bytes,3,opt,name=meta_data,json=metaData,proto3" json:"meta_data,omitempty"`
	//@@  .. cpp:var:: bytes raw_input (repeated)
	//@@
	//@@     The raw input tensor data in the order specified in 'meta_data'.
	//@@
	RawInput             [][]byte `protobuf:"bytes,4,rep,name=raw_input,json=rawInput,proto3" json:"raw_input,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferRequest) Reset()         { *m = InferRequest{} }
func (m *InferRequest) String() string { return proto.CompactTextString(m) }
func (*InferRequest) ProtoMessage()    {}
func (*InferRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{6}
}

func (m *InferRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferRequest.Unmarshal(m, b)
}
func (m *InferRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferRequest.Marshal(b, m, deterministic)
}
func (m *InferRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferRequest.Merge(m, src)
}
func (m *InferRequest) XXX_Size() int {
	return xxx_messageInfo_InferRequest.Size(m)
}
func (m *InferRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_InferRequest.DiscardUnknown(m)
}

var xxx_messageInfo_InferRequest proto.InternalMessageInfo

func (m *InferRequest) GetModelName() string {
	if m != nil {
		return m.ModelName
	}
	return ""
}

func (m *InferRequest) GetModelVersion() int64 {
	if m != nil {
		return m.ModelVersion
	}
	return 0
}

func (m *InferRequest) GetMetaData() *InferRequestHeader {
	if m != nil {
		return m.MetaData
	}
	return nil
}

func (m *InferRequest) GetRawInput() [][]byte {
	if m != nil {
		return m.RawInput
	}
	return nil
}

//@@
//@@.. cpp:var:: message InferResponse
//@@
//@@   Response message for Infer gRPC endpoint.
//@@
type InferResponse struct {
	//@@
	//@@  .. cpp:var:: RequestStatus request_status
	//@@
	//@@     The status of the request, indicating success or failure.
	//@@
	RequestStatus *RequestStatus `protobuf:"bytes,1,opt,name=request_status,json=requestStatus,proto3" json:"request_status,omitempty"`
	//@@  .. cpp:var:: InferResponseHeader meta_data
	//@@
	//@@     The response meta-data for the output tensors.
	//@@
	MetaData *InferResponseHeader `protobuf:"bytes,2,opt,name=meta_data,json=metaData,proto3" json:"meta_data,omitempty"`
	//@@  .. cpp:var:: bytes raw_output (repeated)
	//@@
	//@@     The raw output tensor data in the order specified in 'meta_data'.
	//@@
	RawOutput            [][]byte `protobuf:"bytes,3,rep,name=raw_output,json=rawOutput,proto3" json:"raw_output,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *InferResponse) Reset()         { *m = InferResponse{} }
func (m *InferResponse) String() string { return proto.CompactTextString(m) }
func (*InferResponse) ProtoMessage()    {}
func (*InferResponse) Descriptor() ([]byte, []int) {
	return fileDescriptor_448cf181a38ed067, []int{7}
}

func (m *InferResponse) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_InferResponse.Unmarshal(m, b)
}
func (m *InferResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_InferResponse.Marshal(b, m, deterministic)
}
func (m *InferResponse) XXX_Merge(src proto.Message) {
	xxx_messageInfo_InferResponse.Merge(m, src)
}
func (m *InferResponse) XXX_Size() int {
	return xxx_messageInfo_InferResponse.Size(m)
}
func (m *InferResponse) XXX_DiscardUnknown() {
	xxx_messageInfo_InferResponse.DiscardUnknown(m)
}

var xxx_messageInfo_InferResponse proto.InternalMessageInfo

func (m *InferResponse) GetRequestStatus() *RequestStatus {
	if m != nil {
		return m.RequestStatus
	}
	return nil
}

func (m *InferResponse) GetMetaData() *InferResponseHeader {
	if m != nil {
		return m.MetaData
	}
	return nil
}

func (m *InferResponse) GetRawOutput() [][]byte {
	if m != nil {
		return m.RawOutput
	}
	return nil
}

func init() {
	proto.RegisterType((*StatusRequest)(nil), "nvidia.inferenceserver.StatusRequest")
	proto.RegisterType((*StatusResponse)(nil), "nvidia.inferenceserver.StatusResponse")
	proto.RegisterType((*ProfileRequest)(nil), "nvidia.inferenceserver.ProfileRequest")
	proto.RegisterType((*ProfileResponse)(nil), "nvidia.inferenceserver.ProfileResponse")
	proto.RegisterType((*HealthRequest)(nil), "nvidia.inferenceserver.HealthRequest")
	proto.RegisterType((*HealthResponse)(nil), "nvidia.inferenceserver.HealthResponse")
	proto.RegisterType((*InferRequest)(nil), "nvidia.inferenceserver.InferRequest")
	proto.RegisterType((*InferResponse)(nil), "nvidia.inferenceserver.InferResponse")
}

func init() { proto.RegisterFile("src/core/grpc_service.proto", fileDescriptor_448cf181a38ed067) }

var fileDescriptor_448cf181a38ed067 = []byte{
	// 498 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xb4, 0x54, 0xdd, 0x8a, 0xd3, 0x40,
	0x14, 0x6e, 0xcc, 0x5a, 0xdb, 0xd3, 0xa6, 0xca, 0x5c, 0x2c, 0x25, 0x6b, 0x61, 0x99, 0xdd, 0xd5,
	0xa2, 0x90, 0x95, 0xf5, 0x11, 0x14, 0xb6, 0x05, 0xd1, 0x25, 0x85, 0x85, 0x05, 0x21, 0x8e, 0xc9,
	0x59, 0x37, 0xd0, 0xfc, 0x38, 0x33, 0x6d, 0x9f, 0xc9, 0x37, 0xf0, 0x09, 0x7c, 0x09, 0x5f, 0x46,
	0xe6, 0xa7, 0xa1, 0xc1, 0x0d, 0xed, 0x4d, 0xae, 0x92, 0xf3, 0xf7, 0xcd, 0xf7, 0x9d, 0x39, 0x67,
	0xe0, 0x44, 0xf0, 0xf8, 0x32, 0x2e, 0x38, 0x5e, 0xfe, 0xe0, 0x65, 0x1c, 0x09, 0xe4, 0xeb, 0x34,
	0xc6, 0xa0, 0xe4, 0x85, 0x2c, 0xc8, 0x71, 0xbe, 0x4e, 0x93, 0x94, 0x05, 0x69, 0x7e, 0x8f, 0x1c,
	0xf3, 0x18, 0x55, 0x18, 0xb9, 0x4f, 0xaa, 0x22, 0x56, 0xa6, 0x26, 0xd7, 0x9f, 0x54, 0x3e, 0x8e,
	0x3f, 0x57, 0x28, 0x64, 0x24, 0x24, 0x93, 0x2b, 0x61, 0xc3, 0x2f, 0xab, 0xb0, 0xc1, 0xa8, 0x45,
	0x69, 0x00, 0xde, 0x42, 0xdb, 0xa1, 0xa9, 0x25, 0x13, 0x80, 0xac, 0x48, 0x70, 0x19, 0xe5, 0x2c,
	0xc3, 0xb1, 0x73, 0xea, 0x4c, 0xfb, 0x61, 0x5f, 0x7b, 0x3e, 0xb3, 0x0c, 0xe9, 0x2f, 0x07, 0x46,
	0xdb, 0x02, 0x51, 0x16, 0xb9, 0x40, 0xf2, 0x09, 0x46, 0xf5, 0x83, 0x75, 0xd5, 0xe0, 0xea, 0x22,
	0x78, 0x5c, 0x44, 0x60, 0x8f, 0xb2, 0x30, 0x1e, 0xdf, 0x35, 0xc9, 0x1c, 0xbc, 0x1a, 0xcf, 0xf1,
	0x13, 0x0d, 0x76, 0xde, 0x04, 0xb6, 0xd0, 0x1f, 0x8b, 0x35, 0x14, 0x3b, 0x16, 0xa5, 0x30, 0xba,
	0xe1, 0xc5, 0x7d, 0xba, 0xc4, 0xad, 0xb8, 0x17, 0xe0, 0xc6, 0x59, 0x62, 0x55, 0xa9, 0x5f, 0x1a,
	0xc1, 0xf3, 0x2a, 0xa7, 0x0d, 0x3d, 0xf4, 0x0c, 0xbc, 0x19, 0xb2, 0xa5, 0x7c, 0xd8, 0x72, 0x20,
	0x70, 0xa4, 0xda, 0x69, 0x49, 0xe8, 0x7f, 0xba, 0x86, 0xd1, 0x36, 0xa9, 0x95, 0xa6, 0x1e, 0x43,
	0xf7, 0x41, 0xe3, 0xeb, 0x6e, 0xf6, 0x42, 0x6b, 0xd1, 0xdf, 0x0e, 0x0c, 0xe7, 0x0a, 0xe8, 0xb0,
	0xdb, 0x27, 0x67, 0xe0, 0x99, 0xf0, 0x1a, 0xb9, 0x48, 0x8b, 0x5c, 0xc3, 0xb9, 0xe1, 0x50, 0x3b,
	0x6f, 0x8d, 0x8f, 0x5c, 0x43, 0x3f, 0x43, 0xc9, 0xa2, 0x84, 0x49, 0x36, 0x76, 0x35, 0xeb, 0x37,
	0x4d, 0xac, 0x77, 0x0f, 0x9f, 0x21, 0x4b, 0x90, 0x87, 0x3d, 0x55, 0xfc, 0x91, 0x49, 0x46, 0x4e,
	0xa0, 0xcf, 0xd9, 0x26, 0x4a, 0xf3, 0x72, 0x25, 0xc7, 0x47, 0xa7, 0xee, 0x74, 0x18, 0xf6, 0x38,
	0xdb, 0xcc, 0x95, 0x4d, 0xff, 0x38, 0xe0, 0xd9, 0xea, 0x56, 0x5a, 0x36, 0xdb, 0x55, 0x61, 0x66,
	0xf0, 0xed, 0x1e, 0x15, 0x86, 0xc7, 0x7f, 0x32, 0x26, 0x00, 0x4a, 0x46, 0xb1, 0x92, 0x4a, 0x87,
	0xab, 0x75, 0x28, 0x61, 0x5f, 0xb4, 0xe3, 0xea, 0xaf, 0x0b, 0x83, 0xeb, 0xf0, 0xe6, 0xc3, 0xc2,
	0x3c, 0x00, 0xe4, 0x0e, 0xba, 0x96, 0x42, 0x23, 0xf1, 0xda, 0xc6, 0xfa, 0xaf, 0xf6, 0xa5, 0x19,
	0x5e, 0xb4, 0x43, 0xbe, 0xc2, 0x33, 0x3b, 0xec, 0xa4, 0xb1, 0xa8, 0xbe, 0x31, 0xfe, 0xeb, 0xbd,
	0x79, 0x15, 0xfa, 0x1d, 0x74, 0xcd, 0x10, 0x37, 0x13, 0xaf, 0x6d, 0x42, 0x33, 0xf1, 0xfa, 0x2e,
	0xd0, 0x0e, 0xb9, 0x85, 0xa7, 0xba, 0xc7, 0xe4, 0xfc, 0x90, 0x41, 0xf2, 0x2f, 0x0e, 0xba, 0x28,
	0xda, 0x21, 0xdf, 0x60, 0xb0, 0x90, 0x1c, 0x59, 0xd6, 0x06, 0xfa, 0xd4, 0x79, 0xe7, 0x7c, 0xef,
	0xea, 0x67, 0xf6, 0xfd, 0xbf, 0x00, 0x00, 0x00, 0xff, 0xff, 0xe1, 0xc7, 0x4b, 0x37, 0xee, 0x05,
	0x00, 0x00,
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// GRPCServiceClient is the client API for GRPCService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type GRPCServiceClient interface {
	//@@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
	//@@
	//@@     Get status for entire inference server or for a specified model.
	//@@
	Status(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error)
	//@@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
	//@@
	//@@     Enable and disable low-level GPU profiling.
	//@@
	Profile(ctx context.Context, in *ProfileRequest, opts ...grpc.CallOption) (*ProfileResponse, error)
	//@@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
	//@@
	//@@     Check liveness and readiness of the inference server.
	//@@
	Health(ctx context.Context, in *HealthRequest, opts ...grpc.CallOption) (*HealthResponse, error)
	//@@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
	//@@
	//@@     Request inference using a specific model. [ To handle large input
	//@@     tensors likely need to set the maximum message size to that they
	//@@     can be transmitted in one pass.
	//@@
	Infer(ctx context.Context, in *InferRequest, opts ...grpc.CallOption) (*InferResponse, error)
	//@@  .. cpp:var:: rpc StreamInfer(stream InferRequest) returns (stream
	//@@     InferResponse)
	//@@
	//@@     Request inferences using a specific model in a streaming manner.
	//@@     Individual inference requests sent through the same stream will be
	//@@     processed in order and be returned on completion
	//@@
	StreamInfer(ctx context.Context, opts ...grpc.CallOption) (GRPCService_StreamInferClient, error)
}

type gRPCServiceClient struct {
	cc *grpc.ClientConn
}

func NewGRPCServiceClient(cc *grpc.ClientConn) GRPCServiceClient {
	return &gRPCServiceClient{cc}
}

func (c *gRPCServiceClient) Status(ctx context.Context, in *StatusRequest, opts ...grpc.CallOption) (*StatusResponse, error) {
	out := new(StatusResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Status", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) Profile(ctx context.Context, in *ProfileRequest, opts ...grpc.CallOption) (*ProfileResponse, error) {
	out := new(ProfileResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Profile", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) Health(ctx context.Context, in *HealthRequest, opts ...grpc.CallOption) (*HealthResponse, error) {
	out := new(HealthResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Health", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) Infer(ctx context.Context, in *InferRequest, opts ...grpc.CallOption) (*InferResponse, error) {
	out := new(InferResponse)
	err := c.cc.Invoke(ctx, "/nvidia.inferenceserver.GRPCService/Infer", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *gRPCServiceClient) StreamInfer(ctx context.Context, opts ...grpc.CallOption) (GRPCService_StreamInferClient, error) {
	stream, err := c.cc.NewStream(ctx, &_GRPCService_serviceDesc.Streams[0], "/nvidia.inferenceserver.GRPCService/StreamInfer", opts...)
	if err != nil {
		return nil, err
	}
	x := &gRPCServiceStreamInferClient{stream}
	return x, nil
}

type GRPCService_StreamInferClient interface {
	Send(*InferRequest) error
	Recv() (*InferResponse, error)
	grpc.ClientStream
}

type gRPCServiceStreamInferClient struct {
	grpc.ClientStream
}

func (x *gRPCServiceStreamInferClient) Send(m *InferRequest) error {
	return x.ClientStream.SendMsg(m)
}

func (x *gRPCServiceStreamInferClient) Recv() (*InferResponse, error) {
	m := new(InferResponse)
	if err := x.ClientStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

// GRPCServiceServer is the server API for GRPCService service.
type GRPCServiceServer interface {
	//@@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
	//@@
	//@@     Get status for entire inference server or for a specified model.
	//@@
	Status(context.Context, *StatusRequest) (*StatusResponse, error)
	//@@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
	//@@
	//@@     Enable and disable low-level GPU profiling.
	//@@
	Profile(context.Context, *ProfileRequest) (*ProfileResponse, error)
	//@@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
	//@@
	//@@     Check liveness and readiness of the inference server.
	//@@
	Health(context.Context, *HealthRequest) (*HealthResponse, error)
	//@@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
	//@@
	//@@     Request inference using a specific model. [ To handle large input
	//@@     tensors likely need to set the maximum message size to that they
	//@@     can be transmitted in one pass.
	//@@
	Infer(context.Context, *InferRequest) (*InferResponse, error)
	//@@  .. cpp:var:: rpc StreamInfer(stream InferRequest) returns (stream
	//@@     InferResponse)
	//@@
	//@@     Request inferences using a specific model in a streaming manner.
	//@@     Individual inference requests sent through the same stream will be
	//@@     processed in order and be returned on completion
	//@@
	StreamInfer(GRPCService_StreamInferServer) error
}

// UnimplementedGRPCServiceServer can be embedded to have forward compatible implementations.
type UnimplementedGRPCServiceServer struct {
}

func (*UnimplementedGRPCServiceServer) Status(ctx context.Context, req *StatusRequest) (*StatusResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Status not implemented")
}
func (*UnimplementedGRPCServiceServer) Profile(ctx context.Context, req *ProfileRequest) (*ProfileResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Profile not implemented")
}
func (*UnimplementedGRPCServiceServer) Health(ctx context.Context, req *HealthRequest) (*HealthResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Health not implemented")
}
func (*UnimplementedGRPCServiceServer) Infer(ctx context.Context, req *InferRequest) (*InferResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Infer not implemented")
}
func (*UnimplementedGRPCServiceServer) StreamInfer(srv GRPCService_StreamInferServer) error {
	return status.Errorf(codes.Unimplemented, "method StreamInfer not implemented")
}

func RegisterGRPCServiceServer(s *grpc.Server, srv GRPCServiceServer) {
	s.RegisterService(&_GRPCService_serviceDesc, srv)
}

func _GRPCService_Status_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(StatusRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Status(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Status",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Status(ctx, req.(*StatusRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_Profile_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(ProfileRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Profile(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Profile",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Profile(ctx, req.(*ProfileRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_Health_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(HealthRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Health(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Health",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Health(ctx, req.(*HealthRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_Infer_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(InferRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(GRPCServiceServer).Infer(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/nvidia.inferenceserver.GRPCService/Infer",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(GRPCServiceServer).Infer(ctx, req.(*InferRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _GRPCService_StreamInfer_Handler(srv interface{}, stream grpc.ServerStream) error {
	return srv.(GRPCServiceServer).StreamInfer(&gRPCServiceStreamInferServer{stream})
}

type GRPCService_StreamInferServer interface {
	Send(*InferResponse) error
	Recv() (*InferRequest, error)
	grpc.ServerStream
}

type gRPCServiceStreamInferServer struct {
	grpc.ServerStream
}

func (x *gRPCServiceStreamInferServer) Send(m *InferResponse) error {
	return x.ServerStream.SendMsg(m)
}

func (x *gRPCServiceStreamInferServer) Recv() (*InferRequest, error) {
	m := new(InferRequest)
	if err := x.ServerStream.RecvMsg(m); err != nil {
		return nil, err
	}
	return m, nil
}

var _GRPCService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "nvidia.inferenceserver.GRPCService",
	HandlerType: (*GRPCServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Status",
			Handler:    _GRPCService_Status_Handler,
		},
		{
			MethodName: "Profile",
			Handler:    _GRPCService_Profile_Handler,
		},
		{
			MethodName: "Health",
			Handler:    _GRPCService_Health_Handler,
		},
		{
			MethodName: "Infer",
			Handler:    _GRPCService_Infer_Handler,
		},
	},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "StreamInfer",
			Handler:       _GRPCService_StreamInfer_Handler,
			ServerStreams: true,
			ClientStreams: true,
		},
	},
	Metadata: "src/core/grpc_service.proto",
}
